{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Disaster Tweet Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Kevin Guo, Pranav Sriram, Raymond Yao\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "import utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "#from datasets import train_test_split\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_name = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_phi(text):\n",
    "    input_ids = bert_tokenizer.encode(text, add_special_tokens=True)\n",
    "    X = torch.tensor([input_ids])\n",
    "    with torch.no_grad():\n",
    "        reps = bert_model(X)\n",
    "        return reps.last_hidden_state.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_classifier_phi(text):\n",
    "    reps = bert_phi(text)\n",
    "    #return reps.mean(axis=0)  # Another good, easy option.\n",
    "    return reps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table = pd.read_table('2013_Queensland_Floods_train.tsv')\n",
    "train_table['text'] = train_table['text'].apply(lambda x: re.sub(r'https?\\S+', '', x))\n",
    "\n",
    "dev_table = pd.read_table('2013_Queensland_Floods_dev.tsv')\n",
    "dev_table['text'] = dev_table['text'].apply(lambda x: re.sub(r'https?\\S+', '', x))\n",
    "\n",
    "test_table = pd.read_table('2013_Queensland_Floods_test.tsv')\n",
    "test_table['text'] = test_table['text'].apply(lambda x: re.sub(r'https?\\S+', '', x))\n",
    "\n",
    "\n",
    "train = train_table \n",
    "dev = dev_table \n",
    "test = test_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label\n",
      "0     I just though about the night I went clubbing ...      0\n",
      "1     Looks like its going to be another long night ...      0\n",
      "2     @LaniiBanani hahahaha I just told him id have ...      0\n",
      "3     Off to meeting.... with so called... Baaps of ...      0\n",
      "4              Doubt I'll be getting much sleep tonight      0\n",
      "...                                                 ...    ...\n",
      "6014  RT @GrillTeam: The Queensland government has s...      1\n",
      "6015  Can we have 5 NEMA staff from Nigeria  come to...      1\n",
      "6016  RT @7NewsBrisbane: Foam from rough waves at Al...      1\n",
      "6017  RT @abcsouthqld: Master Electricians Australia...      1\n",
      "6018  RT @HomeLoanKing: Leader of Aussie opposition,...      1\n",
      "\n",
      "[6019 rows x 2 columns]\n",
      "                                                   text  label\n",
      "0     Fuck It.. Chelsea should have been all over Br...      0\n",
      "1     Hey Dana Does @Alistairovereem gets the title ...      0\n",
      "2     @mimstacey @janecaro game over. In most states...      0\n",
      "3     I just made a new word: Awkwul . A mix between...      0\n",
      "4     Nothing like stifling heat to make me want to ...      0\n",
      "...                                                 ...    ...\n",
      "998   Grafton Queensland Flood Peaks at 10.7 Meters:...      1\n",
      "999   Helicopters Deployed to Rescue Flood Victims i...      1\n",
      "1000  RT @maltesemanor: NO! we've suffered enough! M...      1\n",
      "1001  @skeletonunicorn the waters upto the door, my ...      1\n",
      "1002  Queensland's flood crisis deepens as death tol...      1\n",
      "\n",
      "[1003 rows x 2 columns]\n",
      "                                                   text  label\n",
      "0     @MarkSDobson I always thought that, big lad ai...      1\n",
      "1     @Maree_22 And this going to continue until tom...      1\n",
      "2     Room Available in Jacobs Ridge Ormeau $200.00 ...      1\n",
      "3     Dotti Size 6 Sailor/ Air Hostess / Japanese Sc...      1\n",
      "4     @Purpletiger79 I really enjoyed doing a photo ...      1\n",
      "...                                                 ...    ...\n",
      "3006  Thoughts go out to the flood victims of Queens...      1\n",
      "3007  #leimo  Keith Urban's Brisbane, Australia Conc...      1\n",
      "3008  RT @350: No end in sight for Australia flood v...      1\n",
      "3009  Queensland Treasurer Tim Nicholls spoke to Ste...      1\n",
      "3010  RT @GrillTeam: The Queensland government has s...      1\n",
      "\n",
      "[3011 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)\n",
    "print(dev)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6019\n",
      "1003\n"
     ]
    }
   ],
   "source": [
    "X_str_train = train.text.values\n",
    "print(len(X_str_train))\n",
    "y_train = train.label.values\n",
    "\n",
    "X_str_dev = dev.text.values\n",
    "print(len(X_str_dev))\n",
    "y_dev = dev.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 22s, sys: 7.09 s, total: 16min 29s\n",
      "Wall time: 16min 23s\n"
     ]
    }
   ],
   "source": [
    "%time X_train = [bert_classifier_phi(text) for text in X_str_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 1.05 s, total: 2min 45s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%time X_dev = [bert_classifier_phi(text) for text in X_str_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TorchShallowNeuralClassifier(\n",
    "    early_stopping=True,\n",
    "    hidden_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 43. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 0.5618827491998672"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.75 s, sys: 147 ms, total: 8.9 s\n",
      "Wall time: 8.41 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.893     0.972     0.931       462\n",
      "           1      0.974     0.900     0.936       541\n",
      "\n",
      "    accuracy                          0.933      1003\n",
      "   macro avg      0.933     0.936     0.933      1003\n",
      "weighted avg      0.937     0.933     0.933      1003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_dev, preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_data = pd.read_csv('tweets_mod_copy.csv')\n",
    "second_data['text'] = second_data['text'].apply(lambda x: re.sub(r'https?\\S+', '', x))\n",
    "second_train, second_dev, second_test = np.split(second_data.sample(frac=1, random_state=42), [int(.8*len(second_data)), int(.9*len(second_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  labels\n",
      "3495   How many illegal buildings should be demolishe...       0\n",
      "5461                     Whoâ€™s fatality is this tho ????       0\n",
      "9794   #OnThisDay 2018 Chinese state media confirmed ...       1\n",
      "11105  With any luck you will miss the windstorm on e...       0\n",
      "1803   Inferno on Black Friday 1939: 71 deaths, 3,700...       1\n",
      "...                                                  ...     ...\n",
      "2196   go ahead and make a playlist with your name. g...       0\n",
      "8561   Ruckelshaus, Sweeney and DDT â€“ rescued from th...       0\n",
      "11236  ðŸ˜‚We learned a long time ago why all major bank...       0\n",
      "4285   5,000 feral camels culled in drought-hit Austr...       1\n",
      "8569   Another rescued mumma koala with her little ne...       1\n",
      "\n",
      "[9096 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(second_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9096\n",
      "1137\n"
     ]
    }
   ],
   "source": [
    "second_X_str_train = second_train.text.values\n",
    "print(len(second_X_str_train))\n",
    "second_y_train = second_train.labels.values\n",
    "\n",
    "second_X_str_dev = second_dev.text.values\n",
    "print(len(second_X_str_dev))\n",
    "second_y_dev = second_dev.labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 35s, sys: 7.87 s, total: 24min 43s\n",
      "Wall time: 25min 18s\n"
     ]
    }
   ],
   "source": [
    "%time second_X_train = [bert_classifier_phi(text) for text in second_X_str_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 1s, sys: 883 ms, total: 3min 1s\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%time second_X_dev = [bert_classifier_phi(text) for text in second_X_str_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 33. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 1.9881386458873749"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.88 s, sys: 126 ms, total: 10 s\n",
      "Wall time: 9.45 s\n"
     ]
    }
   ],
   "source": [
    "%time _ = model.fit(second_X_train, second_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_preds = model.predict(second_X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.920     0.947     0.933       920\n",
      "           1      0.742     0.650     0.693       217\n",
      "\n",
      "    accuracy                          0.890      1137\n",
      "   macro avg      0.831     0.798     0.813      1137\n",
      "weighted avg      0.886     0.890     0.887      1137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(second_y_dev, second_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
